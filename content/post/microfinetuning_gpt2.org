#+TITLE: micro-finetuning gpt2
#+SUBTITLE: notes on finetuning gpt2 with a tiny dataset
#+AUTHOR: xdrie
#+DATE: 2021-02-07
#+TAGS[]: dev
#+TOC: true

* introduction
** disclaimer
i am primarily a software engineer, and thus am quite an amateur at machine learning. please take my words with a grain of salt, and feel free to [[https://rie.icu/contact.html][contact]] me with any
corrections or suggestions.

this article contains an overview of some of my thoughts, observations,
and experiences with gpt2.

** why gpt2?
it's now been about two years since the
[[https://openai.com/blog/better-language-models/][release of gpt2]].
though it has
[[https://blog.exxactcorp.com/what-can-you-do-with-the-openai-gpt-3-language-model/][largely]]
been [[https://www.gwern.net/newsletter/2020/05#gpt-3][superseded]] by
gpt3, i think gpt2 is still absolutely worth our time for two reasons:

firstly, gpt3 is being [[https://openai.com/blog/openai-api/][withheld]]
from the public, instead being
[[https://openai.com/blog/openai-api/][gated]] behind a
[[https://bdtechtalks.com/2020/09/21/gpt-3-economy-business-model/][prohibitively
expensive]] api. all this from an organization called "openai." not
[[https://www.vice.com/en/article/kzdyme/openais-mission-to-benefit-humanity-now-includes-seeking-profit][so open]] anymore, it would seem. in fact, it recently
[[https://techcrunch.com/2019/03/11/openai-shifts-from-nonprofit-to-capped-profit-to-attract-capital/][transitioned]]
from non-profit to for-profit, and
[[https://www.geekwire.com/2020/openai-renamed-closedai-reaction-microsofts-exclusive-license-openais-gpt-3/][sold
exclusive gpt-3 rights]] to microsoft. a tragedy for open scientific
research, now that one of the most prominent ml research organizations
in recent years has been corrupted by the tech industry. recently, a
glimmer of hope has arrived in eleutherai, who
[[https://www.eleuther.ai/projects/gpt-neo/][promise]] an open, public
gpt-3 pretrained model.

secondly, gpt3, boasting 175 billion parameters (gpt2-xl had 1.5
billion), required an
[[https://web.archive.org/web/20210207084636/https://twitter.com/eturner303/status/1266264358771757057?lang=en][immense]]
amount of resources (14 million usd) to train. this of course puts
training capabilities completely out of reach of all but the
money-hoarding corporations that hold a complete monopoly over the
resources needed for technology research. since the model is simply
immense, the resource cost even for inference is, without a doubt, out
of reach of typical consumer hardware. one reddit thread
[[https://i.reddit.com/r/MachineLearning/comments/idsqnj/discussion_how_much_does_it_cost_to_run_gpt3/g2cx8tm/][suggests]]
that even loading the model takes 350gb of vram. another thread
[[https://i.reddit.com/r/MachineLearning/comments/gzb5uv/d_what_would_it_take_to_run_openais_gpt3_on/fti44lv/][does
the math]] and corroborates the 350gb estimate. a
[[http://web.archive.org/web/20210207085843/https://twitter.com/nickwalton00/status/1294689172804911104][tweet]]
by an [[http://aidungeon.io][aidungeon]] dev (it's fun, and is built on
gpt3) says that a system on par with the
[[http://web.archive.org/web/20200705141625/https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/dgx-1/NVIDIA-DGX-1-Volta-AI-Supercomputer-Datasheet.pdf][dgx1]]
would be required. keep in mind that all this is just for inference.
even this is out of reach for 99% of programmers.

for these reasons, i believe that gpt3 is largely inaccessible, and thus
infeasible for ai-based text generation on a scale accessible to most
developers. gpt2, on the other hand, runs inference excellently on
midrange consumer hardware, and even performs acceptably (for the
smaller variants) on cpu-only machines.

** how to gpt2, hassle free
i wanted a hassle-free, simple, portable way to train, tweak, and run
gpt2. after some basic internet searching, i chose
[[https://github.com/minimaxir/aitextgen][aitextgen]], which provides a
robust python interface built on [[https://pytorch.org/][pytorch]] and
[[https://huggingface.co/transformers/][huggingface transformers]].

aitextgen strongly appeals to me because it perfectly hides all the
messy details under a
[[https://github.com/minimaxir/aitextgen#quick-examples][pleasant]],
intuitive interface, and comes with decent
[[https://docs.aitextgen.io/][documentation]] and
[[https://github.com/minimaxir/aitextgen#demo][colab notebooks]].

* my experiment
** the idea
i wanted to create a gpt2-based generator for text that parodies the
hollow, clinical enthusiasm of corporate pr-speak.

my generator would pretend to be =Delish!â„¢=, a fictional megacorporation
primarily in the food industry. for my dataset, i used a text file of
hand-written announcements. this dataset was fairly small, coming to an
unimpressive 20.5kb and 3190 words.

i decided to use gpt2-s (124m) for two reasons: i wanted my model to run
on low-end cpu-only hardware with acceptable latency, and that i didn't
want to increase the chances of overfitting my model because of the
small dataset size.

one
[[https://towardsdatascience.com/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7][can
use]] [[https://openai.com/blog/fine-tuning-gpt-2/][fine-tuning]] of
gpt2 to get it to generate text better suited to a given domain.
generally, fine-tuning is done on a sizable corpus, approximately in the
range of 5mb to 50mb of text. since our dataset is orders of magnitude
smaller, adjustments are required to fine-tune our model properly.

** source and data

- [[https://gist.github.com/xdrie/18b1acef5b45b037280be79bce11d28c#file-aitextgen_finetune_delish_v1-ipynb][colab notebook]]
- [[https://gist.github.com/xdrie/18b1acef5b45b037280be79bce11d28c#file-delish_txt_v2-txt][data]]
- [[https://github.com/xdrie/aitextgen_host/releases/download/v1.2.0/Delish_v1_ATG_20210206_083350.7z][trained model]]

** micro-fine tuning
*** summary
note that this is not a widely used term, but simply a term i am going
to use to refer to a specific type of fine-tuning.

our goal here is to coax the model to pick up on the stylistic
properties of our small input dataset, striking a balance between
memorizing/repeating the dataset vocabulary and outputting off-topic
text.

generally, we want the model to pick up on the word choice and style in
the training data, and select aspects of its english grammar and
vocabulary that are adjacent to our source text, so as to generate
convincing output text.

*** observations and tips
here i will summarize my experiences trying to micro-finetune gpt2.

i started fine-tuning initially with the default presets, with a
learning rate of =1e-4= and =5000= steps. i noticed that this rapidly
(=<1000=) led to my model simply memorizing the training data, doing
nothing but regurgitating it verbatim.

i lowered my parameters to compensate: i set learning rate to =1e-6= and
steps to =100=. predictably, this led to the model not really seeming to
learn from the dataset, instead preferring to go off topic and spew out
a sliver of internet stuffing.

after some more similar experimentation, i found that a learning rate of
=1e-5= and =400= steps resulted in a model that incorporated just the
right amount of corporate inflection and generated text that was
on-topic with respect to the training data. great!

in summary: if the model is memorizing, you are training too much; if
the model is rambling about unrelated things, you are not training
enough.

* conclusion
both gpt3 and gpt2 without a doubt have
[[https://web.archive.org/web/20200703102314/https://www.gwern.net/GPT-3#weaknesses][many limitations and weaknesses]]. they are still, incredibly versatile,
powerful, and fun models. i am sure there remains a world of possibility
within their constraints, for the time being, until we have something
better in this domain, which is likely, but not inevitable. i strongly
believe in the importance of open, public research, so i hope that
either openai goes back to being open (unlikely, considering the money
rolling in), or that we see another player in the space soon, that makes
openness a priority, for the good of humanity.
